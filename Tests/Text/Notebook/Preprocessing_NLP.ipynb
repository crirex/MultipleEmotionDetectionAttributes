{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab7ec2e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NLTKPreprocessor' from 'nltk' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7100/3053721683.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m### Natural language Toolkit ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwordpunct_tokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNLTKPreprocessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordnet\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'NLTKPreprocessor' from 'nltk' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py)"
     ]
    }
   ],
   "source": [
    "### General imports ###\n",
    "import wget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from random import randint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "import dill\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "from nltk.corpus import movie_reviews as reviews\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import word2vec\n",
    "\n",
    "### Natural language Toolkit ###\n",
    "from nltk import *\n",
    "from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords as sw, wordnet as wn\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "### Scikit-Learn ###\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import precision_score, accuracy_score, confusion_matrix, classification_report as clsr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split as tts\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "### Tensorflow ###\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model, model_from_json\n",
    "from tensorflow.keras.layers import (Dense, LSTM, SpatialDropout1D, Activation, Conv1D, MaxPooling1D, \n",
    "                                     Input, concatenate, Embedding, BatchNormalization)\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7c006e",
   "metadata": {},
   "source": [
    "# 1. Essays (Stream-of-consciousness Essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a00cba",
   "metadata": {},
   "source": [
    "### 1.1 Dataset informations\n",
    "Essays is a dataset created by J. W. Pennebaker and L. A. King in their paper “Linguistic styles: Language use as an individual difference” where they explore daily diaries from 15 people with substance abuse, 35 from students and journal abstracts from 40 social psychologists to determine weather or not the text written by a person could reflect their personality (https://paperswithcode.com/dataset/essays).\n",
    "\n",
    "The identifiers for the dataset csv that we used are:\n",
    "- **id** (id of the entry)\n",
    "- **text** (the text associated with the entry)\n",
    "- **extraversion** (y = extrovert, n = introvert)\n",
    "- **neuroticism** (y = neurotic, n = tranquil)\n",
    "- **agreeableness** (y = agreeable, n = disagreeable)\n",
    "- **conscientiousness** (y = conscientious, n = casual)\n",
    "- **openness** (y = open, n = closed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adad9570",
   "metadata": {},
   "source": [
    "### 1.2 Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c5869ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../Data/essays.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f97e3208",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_essays = pd.read_csv(file_path, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2c12834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>openness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997_504851.txt</td>\n",
       "      <td>Well, right now I just woke up from a mid-day ...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997_605191.txt</td>\n",
       "      <td>Well, here we go with the stream of consciousn...</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997_687252.txt</td>\n",
       "      <td>An open keyboard and buttons to push. The thin...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997_568848.txt</td>\n",
       "      <td>I can't believe it!  It's really happening!  M...</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997_688160.txt</td>\n",
       "      <td>Well, here I go with the good old stream of co...</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>2004_493.txt</td>\n",
       "      <td>I'm home. wanted to go to bed but remembe...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>2004_494.txt</td>\n",
       "      <td>Stream of consiousnesssskdj. How do you s...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>2004_497.txt</td>\n",
       "      <td>It is Wednesday, December 8th and a lot has be...</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>2004_498.txt</td>\n",
       "      <td>Man this week has been hellish. Anyways, now i...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>2004_499.txt</td>\n",
       "      <td>I have just gotten off the phone with brady. I...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2467 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               text  \\\n",
       "0     1997_504851.txt  Well, right now I just woke up from a mid-day ...   \n",
       "1     1997_605191.txt  Well, here we go with the stream of consciousn...   \n",
       "2     1997_687252.txt  An open keyboard and buttons to push. The thin...   \n",
       "3     1997_568848.txt  I can't believe it!  It's really happening!  M...   \n",
       "4     1997_688160.txt  Well, here I go with the good old stream of co...   \n",
       "...               ...                                                ...   \n",
       "2462     2004_493.txt       I'm home. wanted to go to bed but remembe...   \n",
       "2463     2004_494.txt       Stream of consiousnesssskdj. How do you s...   \n",
       "2464     2004_497.txt  It is Wednesday, December 8th and a lot has be...   \n",
       "2465     2004_498.txt  Man this week has been hellish. Anyways, now i...   \n",
       "2466     2004_499.txt  I have just gotten off the phone with brady. I...   \n",
       "\n",
       "     extraversion neuroticism agreeableness conscientiousness openness  \n",
       "0               n           y             y                 n        y  \n",
       "1               n           n             y                 n        n  \n",
       "2               n           y             n                 y        y  \n",
       "3               y           n             y                 y        n  \n",
       "4               y           n             y                 n        y  \n",
       "...           ...         ...           ...               ...      ...  \n",
       "2462            n           y             n                 y        n  \n",
       "2463            y           y             n                 n        y  \n",
       "2464            n           n             y                 n        n  \n",
       "2465            n           y             n                 n        y  \n",
       "2466            n           y             y                 n        y  \n",
       "\n",
       "[2467 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644f1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary download for nltk in case it's not already downloaded\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c74b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data processing\n",
    "data_essays['extraversion'] = np.where(data_essays['extraversion']=='y', 1, 0)\n",
    "data_essays['neuroticism'] = np.where(data_essays['neuroticism']=='y', 1, 0)\n",
    "data_essays['agreeableness'] = np.where(data_essays['agreeableness']=='y', 1, 0)\n",
    "data_essays['conscientiousness'] = np.where(data_essays['conscientiousness']=='y', 1, 0)\n",
    "data_essays['openness'] = np.where(data_essays['openness']=='y', 1, 0)\n",
    "X_essays = data_essays['text'].tolist()\n",
    "y_essays = data_essays[['extraversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']]\n",
    "data_essays['text_length'] = data_essays['text'].apply(len)\n",
    "labels = ['extraversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59054ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and tokenize the texts\n",
    "complete_corpus = ' '.join(X_essays)\n",
    "words = tokenize.word_tokenize(complete_corpus)\n",
    "fdist = FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da8ee688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of essays: 2467\n",
      "The total number of words in all essays: 1608813\n",
      "The average number of words in each essay: 652.1333603567085\n"
     ]
    }
   ],
   "source": [
    "number_of_words = [len(text.split()) for text in X_essays]\n",
    "\n",
    "print('The total number of essays: {}'.format(len(X_essays)))\n",
    "print('The total number of words in all essays: {}'.format(sum(number_of_words)))\n",
    "print('The average number of words in each essay: {}'.format(sum(number_of_words)/len(number_of_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60907099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 100 most frequent words/counts: [('I', 115487), ('.', 111178), ('to', 56263), (',', 47355), ('the', 38232), ('and', 36810), ('that', 29456), ('a', 28412), ('my', 26580), ('is', 25576), ('of', 22939), ('it', 22782), (\"n't\", 19996), ('in', 17828), ('do', 17448), ('have', 16166), ('me', 14588), ('so', 13099), ('but', 13060), ('this', 12054), ('be', 11724), ('for', 11520), (\"'s\", 11198), ('was', 10392), ('am', 10378), ('like', 10308), ('just', 10250), ('really', 10207), ('not', 10015), (\"'m\", 9973), ('on', 9015), ('about', 8941), ('with', 8708), ('think', 8061), ('are', 7602), ('what', 7517), ('all', 7475), ('at', 7469), ('because', 7144), ('i', 7048), ('know', 6959), ('get', 6875), ('he', 6605), ('now', 6154), ('would', 6077), ('you', 6013), ('if', 6001), ('time', 5966), ('out', 5923), ('they', 5905), ('up', 5743), ('or', 5733), ('going', 5621), ('go', 5576), ('she', 5556), ('?', 5539), ('want', 5483), ('will', 5420), ('can', 5276), ('!', 4959), ('as', 4939), ('people', 4898), ('her', 4879), ('when', 4867), ('we', 4836), ('much', 4702), ('It', 4626), ('one', 4465), ('how', 4308), ('feel', 4239), ('there', 4219), ('him', 3988), ('good', 3964), ('here', 3837), ('more', 3810), ('some', 3762), ('had', 3759), ('from', 3684), ('need', 3580), ('been', 3560), ('right', 3428), ('them', 3346), ('did', 3323), ('ca', 3261), ('too', 3213), ('has', 3200), ('could', 3131), ('things', 3117), ('My', 3075), ('well', 3012), ('school', 3010), ('class', 2983), ('wonder', 2959), ('see', 2882), ('should', 2836), ('guess', 2811), ('friends', 2804), ('back', 2684), ('something', 2657), ('very', 2648)]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessed words from the essays\n",
    "print(\"List of 100 most frequent words/counts: {}\".format(fdist.most_common(100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3306943e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NLTKPreprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7100/65170315.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_preprocess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNLTKPreprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'NLTKPreprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "X_preprocess = NLTKPreprocessor.transform(self.X).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa07378f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006ae3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d4832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24abfe12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a7be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d088fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2e1b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a439b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06de965",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(X_essays, y_essays, test_size=0.2)\n",
    "\n",
    "# Train-test split to save the dataset\n",
    "with open('./Data/X_train.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "with open('./Data/X_test.pkl', 'wb') as f:\n",
    "    pickle.dump(X_test, f)\n",
    "with open('./Data/y_train.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "with open('./Data/y_test.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "\n",
    "# Load train and test sets from pickled lists\n",
    "with open('./Data/X_train.pkl', 'rb') as pickle_file:\n",
    "    X_train = pickle.load(pickle_file)\n",
    "with open('./Data/X_test.pkl', 'rb') as pickle_file:\n",
    "    X_test = pickle.load(pickle_file)\n",
    "with open('./Data/y_train.pkl', 'rb') as pickle_file:\n",
    "    y_train = pickle.load(pickle_file)\n",
    "with open('./Data/y_test.pkl', 'rb') as pickle_file:\n",
    "    y_test = pickle.load(pickle_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
